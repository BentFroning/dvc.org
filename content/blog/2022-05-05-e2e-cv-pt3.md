---
title:
  'End-to-End Computer Vision API, Part 3: Remote Experiments & CI/CD For
  Machine Learning'
date: 2022-05-05
description: >
  This is the third and last part of this series of posts on how to build an
  end-to-end Computer Vision solution according to the best available MLOps
  practices. 

  In part 1, we talked about exploratory work in Jupyter Notebooks; versioning
  data in remote storage with DVC; and refactoring the code from Jupyter
  Notebooks into DVC pipeline stages.

  Part 2 talked about the process of managing experiments with DVC pipelines,
  DVCLive and Iterative Studio.

descriptionLong: |

  This is the third and last part of this series of posts on how to build an end-to-end Computer Vision solution according to the best available MLOps practices. 

  In part 1, we talked about exploratory work in Jupyter Notebooks; versioning data in remote storage with DVC; and refactoring the code from Jupyter Notebooks into DVC pipeline stages.

  Part 2 talked about the process of managing experiments with DVC pipelines, DVCLive and Iterative Studio.

picture: <TBD>
author: alex_kim
commentsUrl: https://discuss.dvc.org/t/end-to-end-computer-vision/1178
tags:
  - Computer Vision
  - DVC
  - CML
  - Studio
  - CI/CD
  - Experiment Tracking
---

### Introduction

In this post, we’ll focus on leveraging cloud infrastructure with CML and how to
enable automatic reporting (with metrics, plots, and other visuals) to accompany
every commit/pull request on GitHub.

### Leveraging Cloud Resources with CI/CD and CML

If you ever wondered if there's an easy way to quickly and easily:

1. provision a powerful VM in the cloud
2. submit your ML training job to it
3. get the results back
4. automatically shut down the VM without having to worry about excessive cloud
   bills

then the answer is yes if you use the [CML library](https://cml.dev/) in
combination with CI/CD tools like GitHub Actions or GitLab CI/CD.

![Continuous Integration and Deployment for Machine Learning](https://dvc.org/static/300c88b3b1b5f65753629d661cc916e5/2e49e/cicd4ml.png)

We've configured three
[workflow files](https://github.com/iterative/magnetic-tiles-defect/tree/main/.github/workflows)
for GitHub Actions, each of which corresponds to a particular stage depending on
the project's lifecycle we are in:

1. [Workflow for experimentation and hyperparameter tuning](https://github.com/iterative/magnetic-tiles-defect/blob/main/.github/workflows/1-experiment.yaml)

   ![Workflow for experimentation and hyperparameter tuning](/uploads/images/2022-05-05/workflow_exp.png '=800')

   In this stage, we'll be working on an experimentation git branch:
   experimenting with data preprocessing, changing model architecture, tuning
   hyperparameters, etc. Once we think our experiment is ready to be run, we'll
   push our changes to a remote repository (in this case, GitHub). This push
   will trigger a CI/CD job in GitHub Actions, which in turn will:

   1. provision an EC2 virtual machine with a GPU in AWS:

      ```yaml
      ...
      - name: deploy
              env:
                REPO_TOKEN: ${{ secrets.PERSONAL_ACCESS_TOKEN }}
              run: |
                cml runner \
                    --cloud=aws \
                    --cloud-region=us-east-1 \
                    --cloud-type=g4dn.xlarge \
                    --labels=cml-runner
      ...
      ```

   2. deploy our experiment branch to a docker container on this machine:

      ```yaml
      ---
      train-model:
        needs: deploy-runner
        runs-on: [self-hosted, cml-runner]
        container:
          image: iterativeai/cml:0-dvc2-base1
          options: --gpus all
        environment: cloud
        permissions:
          contents: read
          id-token: write
        steps:
          - uses: actions/checkout@v2
          - uses: actions/setup-python@v2
            with:
              python-version: '3.9'
      ```

   3. rerun the entire DVC pipeline & push metrics back to GitHub;

      ```yaml
      ...
      - name: dvc-repro-cml
              env:
                REPO_TOKEN: ${{ secrets.PERSONAL_ACCESS_TOKEN }}
              run: |
                # Install dependencies
                pipenv install --skip-lock
                pipenv run dvc pull
                pipenv run dvc exp run
                pipenv run dvc push
      ...
      ```

   4. open a pull request and post a report to it that contains a table with
      metrics and model outputs (segmentation masks) on a few test images:

      ```yaml
      ...
      # Open a pull request
      cml pr dvc.lock metrics.json training_metrics.json training_metrics_dvc_plots/**
      # Create CML report
      echo "## Metrics" >> report.md
      pipenv run dvc metrics show --md >> report.md
      echo "## A few random test images" >> report.md
      for file in $(ls data/test_preds/ |sort -R |tail -20); do
        cml publish data/test_preds/$file --md >> report.md
      done
      cml send-comment --pr --update report.md
      ...
      ```

      The report structure is fully customizable. Below is an example of what
      the PR and the CML report would look like in this case:

      ![PR and CML report](/uploads/images/2022-05-05/pr_cml_report.png '=800')

   At this point, we can assess the results in Iterative Studio and GitHub and
   decide whether we want to accept the PR or keep experimenting.

2. [Workflow for deploying to the development environment](https://github.com/iterative/magnetic-tiles-defect/blob/main/.github/workflows/2-develop.yaml)

   ![Workflow for deploying to the development environment](/uploads/images/2022-05-05/workflow_dev.png '=800')

   Once we are happy with our model's performance on the experiment branch, we
   can merge it into the development branch. This would trigger a different
   CI/CD job that will:

   1. retrain the model with the new parameters (as the `dev` branch might
      contain changes not present in the `exp` branch). This step looks almost
      identical to the third step in the above workflow.
   2. deploy the web REST API application (that incorporates the new model) to a
      development endpoint on Heroku:

      ```yaml
      ---
      deploy-dev-api:
        needs: train-and-push
        runs-on: ubuntu-latest
        steps:
          - uses: actions/checkout@v2
          - uses: actions/download-artifact@master
            with:
              name: model_pickle
              path: models
          - uses: akhileshns/heroku-deploy@v3.12.12
            with:
              heroku_api_key: ${{secrets.HEROKU_API_KEY}}
              heroku_app_name: 'demo-api-mag-tiles-dev'
              heroku_email: 'alexkim@iterative.ai'
              team: 'iterative-sandbox'
              usedocker: true
      ```

      Now we can test our API and assess the end-to-end performance of the
      overall solution.

3. [Workflow for deploying to the production environment](https://github.com/iterative/magnetic-tiles-defect/blob/main/.github/workflows/3-deploy.yaml)

   ![Workflow for deploying to the production environment](/uploads/images/2022-05-05/workflow_prod.png '=800')

   If we've thoroughly tested and monitored the performance of our development
   web API, we can merge the development branch into the main branch of our
   repository. Again, this triggers the third CI/CD workflow that deploys the
   code from the main branch to the production API. This looks identical to the
   deployment into the development environment except for `heroku_app_name`
   which is now set to `"demo-api-mag-tiles-prod"`.

## Summary

In this three-part blog post, we described how we addressed the problem of
building a Computer Vision Web API for defect detection. We’ve chosen this
approach because it addresses the common challenges that are shared across many
CV projects: how to version datasets that consist of a large number of small- to
medium-sized files; how to avoid triggering long-running stages of an ML
pipeline when it’s not needed for reproducibility; how to run model training
jobs on the cloud infrastructure without having to provision and manage
everything yourself; and, finally, how to track progress in key metrics when you
run many ML experiments.

We've talked about the following:

- Common difficulties when building Computer Vision Web API for defect detection
- Pros and cons of exploratory work in Jupyter Notebooks
- Versioning data in remote storage with DVC
- Moving and refactoring the code from Jupyter Notebooks into DVC pipeline
  stages
- Running experiments locally as well as remotely on cloud infrastructure with
  CML
- Tracking and visualizing experiment results in a central place with Studio

## What to Try Next

- Reproduce this solution by setting your own configs, tokens, and access keys
  to GitHub, AWS, and Heroku
- Add a few simple unit tests and update CML workflow files to run them before
  reproducing the pipeline
- Apply this approach to a different Computer Vision problem using a different
  dataset or different problem type (image classification, object detection,
  etc)
