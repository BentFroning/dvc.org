# Get Started

DVCLive is a Python library for logging machine learning parameters, metrics and
other metadata in simple file formats, which is fully compatible with DVC.

## Set up DVCLive

First of all, you need to add DVCLive to your Python code in order to start
logging metrics.

In this example we are using [Keras](/doc/dvclive/ml-frameworks/keras) for
training our model and we can add DVCLive as follows:

```python
# train.py
from dvclive.keras import DvcLiveCallback
...
model.fit(
  train_dataset, validation_data=validation_dataset,
  callbacks=[DvcLiveCallback(path="training_metrics")])
```

You can find example snippets and reference of the options available in the
corresponding page of any other of the supported
[ML Frameworks](/doc/dvclive/ml-frameworks).

If you prefer to use the DVCLive API directly, check the
[API Reference](/doc/dvclive/api-reference).

## Set up DVC

In order to visualize the outputs generated by DVCLive, we can set up a DVC
pipeline.

First, we initialize DVC with `dvc init`:

```dvc
$ dvc init
```

After that, we use `dvc exp init --live` to create a `dvc.yaml` file:

<admon type="info">

Note that the path passed to the `--live` option (`"training_metrics"`) matches
the `path` passed to the callback in the Python code.

</admon>

```dvc
$ dvc exp init \
--live "training_metrics" \
--code "train.py" \
python train.py
```

<details>

### ⚙️ Expand to see the `dvc.yaml`

The `dvc.yaml` will contain a new `train` stage using the DVCLive outputs as
`metrics` and `plots`:

```yaml
stages:
  train:
    cmd: python train.py
    deps:
      - train.py
    metrics:
      - training_metrics.json:
          cache: false
    plots:
      - training_metrics/scalars:
          cache: false
```

You can find more details about the structure of files generated by DVCLive in
the [Output folder structure](/doc/dvclive/outputs)

</details>

## Run experiments

You can now
[run experiments](/doc/user-guide/experiment-management/running-experiments)
with `dvc exp run`:

```dvc
$ dvc exp run
```

## Compare and visualize experiments

After following the above steps we have enabled different ways of monitoring the
training, comparing, and visualizing the experiments:

### DVC

After the experiment has finished, you can use `dvc exp` / `dvc plots` commands
to compare and visualize experiments.

While the experiment is running, you can monitor the training with the
[metrics report](/docs/dvclive/api-reference#metrics-report) that DVCLive
generates at `training_metrics/report.html`:

![HTML report](/img/dvclive-html.gif).

### Visual Studio Code

If you have installed the
[DVC Extension for Visual Studio Code](https://marketplace.visualstudio.com/items?itemName=Iterative.dvc),
you can monitor the training and compare and visualize experiments using the
[`Experiments`](https://github.com/iterative/vscode-dvc/blob/main/extension/resources/walkthrough/experiments-table.md)
and
[`Plots`](https://github.com/iterative/vscode-dvc/blob/main/extension/resources/walkthrough/plots.md)
views.

![Experiments view](https://github.com/iterative/vscode-dvc/raw/main/extension/resources/walkthrough/images/experiments-table.png)
![Plost view](https://github.com/iterative/vscode-dvc/raw/main/extension/resources/walkthrough/images/plots-trends.png)

### Iterative Studio

After the experiment has finished and you have committed and pushed the results,
[Iterative Studio](/doc/studio) will automatically parse the outputs generated
by DVCLive, allowing to
[compare and visualize experiments](/doc/studio/user-guide/projects-and-experiments/visualize-and-compare):

![Studio view](/img/dvclive-studio-plots.png)
